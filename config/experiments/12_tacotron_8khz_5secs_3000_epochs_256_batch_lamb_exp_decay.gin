include 'config/experiments/tacotron_8khz.gin'

datasets.log_compression = False

batch_size = 256
train_driver.epochs = 3000
train_driver.save_every_epochs = 100
train_driver.optimizer = @tfa.optimizers.LAMB()

tfa.optimizers.LAMB.learning_rate = @WarmUp()
tfa.optimizers.LAMB.clipnorm = 1.0

initial_learning_rate = 2.8284e-3

WarmUp.initial_learning_rate = %initial_learning_rate
WarmUp.warmup_steps = 120
WarmUp.decay_schedule_fn = @tf.keras.optimizers.schedules.PiecewiseConstantDecay()

tf.keras.optimizers.schedules.ExponentialDecay.initial_learning_rate = %initial_learning_rate
tf.keras.optimizers.schedules.ExponentialDecay.decay_steps = 14400
tf.keras.optimizers.schedules.ExponentialDecay.decay_rate = 0.5
